{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install singlestoredb openai tiktoken beautifulsoup4 pandas python-dotenv Markdown praw PyGithub tweepy --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import openai\n",
    "import tiktoken\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import singlestoredb as s2\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "from github import Github\n",
    "from github import Auth\n",
    "import tweepy\n",
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_LIMIT = 10\n",
    "\n",
    "DB_NAME = 'llm_recommender'\n",
    "DB_CONNECTION_URL = f'/{DB_NAME}'\n",
    "\n",
    "OPENAI_API_KEY = ''\n",
    "\n",
    "HF_TOKEN = ''\n",
    "\n",
    "REDDIT_USERNAME = ''\n",
    "REDDIT_PASSWORD = ''\n",
    "REDDIT_CLIENT_ID = ''\n",
    "REDDIT_CLIENT_SECRET = ''\n",
    "REDDIT_USER_AGENT = 'llm_recommender_1.0'\n",
    "\n",
    "GITHUB_ACCESS_TOKEN = ''\n",
    "TWITTER_BEARER_TOKEN = ''\n",
    "\n",
    "TOKENS_LIMIT = 2047\n",
    "TOKENS_TRASHHOLD_LIMIT = TOKENS_LIMIT - 128\n",
    "\n",
    "MODELS_TABLE_NAME = 'models'\n",
    "MODEL_READMES_TABLE_NAME = 'model_readmes'\n",
    "MODEL_TWITTER_POSTS_TABLE_NAME = 'model_twitter_posts'\n",
    "MODEL_REDDIT_POSTS_TABLE_NAME = 'model_reddit_posts'\n",
    "MODEL_GITHUB_REPOS_TABLE_NAME = 'model_github_repos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = s2.connect(DB_CONNECTION_URL)\n",
    "\n",
    "\n",
    "def create_tables():\n",
    "    def create_models_table():\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(f'''\n",
    "                CREATE TABLE IF NOT EXISTS {MODELS_TABLE_NAME} (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    name VARCHAR(512) NOT NULL,\n",
    "                    author VARCHAR(512) NOT NULL,\n",
    "                    repo_id VARCHAR(1024) NOT NULL,\n",
    "                    score DECIMAL(5, 2) NOT NULL,\n",
    "                    arc DECIMAL(5, 2) NOT NULL,\n",
    "                    hellaswag DECIMAL(5, 2) NOT NULL,\n",
    "                    mmlu DECIMAL(5, 2) NOT NULL,\n",
    "                    truthfulqa DECIMAL(5, 2) NOT NULL,\n",
    "                    winogrande DECIMAL(5, 2) NOT NULL,\n",
    "                    gsm8k DECIMAL(5, 2) NOT NULL,\n",
    "                    link VARCHAR(255) NOT NULL,\n",
    "                    downloads INT,\n",
    "                    likes INT,\n",
    "                    still_on_hub BOOLEAN NOT NULL,\n",
    "                    created_at TIMESTAMP,\n",
    "                    embedding BLOB\n",
    "                )\n",
    "            ''')\n",
    "\n",
    "    def create_model_readmes_table():\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(f'''\n",
    "                CREATE TABLE IF NOT EXISTS {MODEL_READMES_TABLE_NAME} (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    model_repo_id VARCHAR(512),\n",
    "                    text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
    "                    clean_text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
    "                    created_at TIMESTAMP,\n",
    "                    embedding BLOB\n",
    "                )\n",
    "            ''')\n",
    "\n",
    "    def create_model_twitter_posts_table():\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(f'''\n",
    "                CREATE TABLE IF NOT EXISTS {MODEL_TWITTER_POSTS_TABLE_NAME} (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    model_repo_id VARCHAR(512),\n",
    "                    post_id VARCHAR(256),\n",
    "                    clean_text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
    "                    created_at TIMESTAMP,\n",
    "                    embedding BLOB\n",
    "                )\n",
    "            ''')\n",
    "\n",
    "    def create_model_reddit_posts_table():\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(f'''\n",
    "                CREATE TABLE IF NOT EXISTS {MODEL_REDDIT_POSTS_TABLE_NAME} (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    model_repo_id VARCHAR(512),\n",
    "                    post_id VARCHAR(256),\n",
    "                    title VARCHAR(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
    "                    clean_text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
    "                    link VARCHAR(256),\n",
    "                    created_at TIMESTAMP,\n",
    "                    embedding BLOB\n",
    "                )\n",
    "            ''')\n",
    "\n",
    "    def create_model_github_repos_table():\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(f'''\n",
    "                CREATE TABLE IF NOT EXISTS {MODEL_GITHUB_REPOS_TABLE_NAME} (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    model_repo_id VARCHAR(512),\n",
    "                    repo_id INT,\n",
    "                    name VARCHAR(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
    "                    description TEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
    "                    clean_text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
    "                    link VARCHAR(256),\n",
    "                    created_at TIMESTAMP,\n",
    "                    embedding BLOB\n",
    "                )\n",
    "            ''')\n",
    "\n",
    "    create_models_table()\n",
    "    create_model_readmes_table()\n",
    "    create_model_twitter_posts_table()\n",
    "    create_model_reddit_posts_table()\n",
    "    create_model_github_repos_table()\n",
    "\n",
    "\n",
    "def drop_table(table_name: str):\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(f'DROP TABLE IF EXISTS {DB_NAME}.{table_name}')\n",
    "\n",
    "\n",
    "def get_models(select='*', query='', as_dict=True):\n",
    "    with connection.cursor() as cursor:\n",
    "        _query = f'SELECT {select} FROM {MODELS_TABLE_NAME}'\n",
    "\n",
    "        if query:\n",
    "            _query += f' {query}'\n",
    "\n",
    "        cursor.execute(_query)\n",
    "\n",
    "        if as_dict:\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            return [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
    "\n",
    "        return cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "def count_tokens(text: str):\n",
    "    enc = tiktoken.get_encoding('cl100k_base')\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "\n",
    "def create_embedding(input):\n",
    "    data = openai.embeddings.create(input=input, model='text-embedding-ada-002').data\n",
    "    return data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "def string_into_chunks(string: str, max_tokens=TOKENS_LIMIT):\n",
    "    if count_tokens(string) <= max_tokens:\n",
    "        return [string]\n",
    "\n",
    "    delimiter = ' '\n",
    "    words = string.split(delimiter)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        if count_tokens(delimiter.join(current_chunk + [word])) <= max_tokens:\n",
    "            current_chunk.append(word)\n",
    "        else:\n",
    "            chunks.append(delimiter.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(delimiter.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def clean_string(string: str):\n",
    "    def strip_html_elements(string: str):\n",
    "        html = markdown(string)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "        return text.strip()\n",
    "\n",
    "    def remove_unicode_escapes(string: str):\n",
    "        return re.sub(r'[^\\x00-\\x7F]+', '', string)\n",
    "\n",
    "    def remove_string_spaces(strgin: str):\n",
    "        new_string = re.sub(r'\\n+', '\\n', strgin)\n",
    "        new_string = re.sub(r'\\s+', ' ', new_string)\n",
    "        return new_string\n",
    "\n",
    "    def remove_links(string: str):\n",
    "        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "        return re.sub(url_pattern, '', string)\n",
    "\n",
    "    new_string = strip_html_elements(string)\n",
    "    new_string = remove_unicode_escapes(new_string)\n",
    "    new_string = remove_string_spaces(new_string)\n",
    "    new_string = re.sub(r'\\*\\*+', '*', new_string)\n",
    "    new_string = re.sub(r'--+', '-', new_string)\n",
    "    new_string = re.sub(r'====+', '=', new_string)\n",
    "    new_string = remove_links(new_string)\n",
    "\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaderboard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaderboard_get_df():\n",
    "    url = 'https://raw.githubusercontent.com/singlestore-labs/llm-recommender/main/backend/leaderboard/datasets/leaderboard.json?token=GHSAT0AAAAAACLI5LF64INSSDY7POI6CXF2ZM4LNOA'\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = json.loads(response.text)\n",
    "        df = pd.DataFrame(data).head(MODELS_LIMIT)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve JSON file\")\n",
    "\n",
    "\n",
    "def leaderboard_get_models():\n",
    "    models = []\n",
    "    existed_model_repo_ids = [i[0] for i in get_models('repo_id', as_dict=False)]\n",
    "    leaderboard_df = leaderboard_get_df()\n",
    "\n",
    "    for i, row in leaderboard_df.iterrows():\n",
    "        if row['repo_id'] in existed_model_repo_ids:\n",
    "            continue\n",
    "\n",
    "        models.append(row.to_dict())\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def leaderboard_insert_models(models):\n",
    "    if not len(models):\n",
    "        return\n",
    "\n",
    "    _models = []\n",
    "    readmes = []\n",
    "\n",
    "    for model in models:\n",
    "        model['created_at'] = model['created_at']\n",
    "        _model = {key: value for key, value in model.items() if key != 'readme'}\n",
    "        to_embedding = json.dumps(_model, cls=JSONEncoder)\n",
    "        embedding = str(create_embedding(to_embedding))\n",
    "        _models.append({**_model, embedding: embedding})\n",
    "\n",
    "        if not model['readme']:\n",
    "            continue\n",
    "\n",
    "        readme = {\n",
    "            'model_repo_id': model['repo_id'],\n",
    "            'text': model['readme'],\n",
    "            'created_at': time()\n",
    "        }\n",
    "\n",
    "        if count_tokens(readme['text']) <= TOKENS_TRASHHOLD_LIMIT:\n",
    "            readme['clean_text'] = clean_string(readme['text'])\n",
    "            to_embedding = json.dumps({\n",
    "                'model_repo_id': readme['model_repo_id'],\n",
    "                'clean_text': readme['clean_text'],\n",
    "            })\n",
    "            readme['embedding'] = str(create_embedding(to_embedding))\n",
    "            readmes.append(readme)\n",
    "        else:\n",
    "            for i, chunk in enumerate(string_into_chunks(readme['text'])):\n",
    "                _readme = {\n",
    "                    **readme,\n",
    "                    'text': chunk,\n",
    "                    'created_at': time()\n",
    "                }\n",
    "\n",
    "                _readme['clean_text'] = clean_string(chunk)\n",
    "                to_embedding = json.dumps({\n",
    "                    'model_repo_id': _readme['model_repo_id'],\n",
    "                    'clean_text': chunk,\n",
    "                })\n",
    "                _readme['embedding'] = str(create_embedding(to_embedding))\n",
    "                readmes.append(_readme)\n",
    "\n",
    "    with connection.cursor() as cursor:\n",
    "        model_values = [tuple(model.values()) for model in _models]\n",
    "        cursor.executemany(f'''\n",
    "            INSERT INTO {MODELS_TABLE_NAME} (name, author, repo_id, score, link, still_on_hub, arc, hellaswag, mmlu, truthfulqa, winogrande, gsm8k, downloads, likes, created_at, embedding)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, FROM_UNIXTIME(%s), JSON_ARRAY_PACK(%s))\n",
    "        ''', model_values)\n",
    "\n",
    "        readme_values = [tuple(readme.values()) for readme in readmes]\n",
    "        cursor.executemany(f'''\n",
    "            INSERT INTO {MODEL_READMES_TABLE_NAME} (model_repo_id, text, created_at, clean_text, embedding)\n",
    "            VALUES (%s, %s, FROM_UNIXTIME(%s), %s, JSON_ARRAY_PACK(%s))\n",
    "        ''', readme_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitHub setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github = Github(auth=Auth.Token(GITHUB_ACCESS_TOKEN))\n",
    "\n",
    "\n",
    "def github_search_repos(keyword: str, last_created_at):\n",
    "    repos = []\n",
    "    query = f'\"{keyword}\" in:name,description,readme'\n",
    "\n",
    "    if last_created_at:\n",
    "        query += f' created:>{last_created_at}'\n",
    "\n",
    "    try:\n",
    "        for repo in github.search_repositories(query):\n",
    "            try:\n",
    "                readme_file = repo.get_readme()\n",
    "\n",
    "                if readme_file.size > 7000:\n",
    "                    continue\n",
    "\n",
    "                readme = readme_file.decoded_content.decode('utf-8')\n",
    "\n",
    "                repos.append({\n",
    "                    'repo_id': repo.id,\n",
    "                    'name': repo.name,\n",
    "                    'link': repo.html_url,\n",
    "                    'created_at': repo.created_at.timestamp(),\n",
    "                    'description': repo.description if bool(repo.description) else '',\n",
    "                    'readme': readme,\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print('GitHub search repos error: ', e,  f'\\nmodel: {keyword}', f'\\nquery: {query}')\n",
    "        return repos\n",
    "\n",
    "    return repos\n",
    "\n",
    "\n",
    "def github_get_models_repos(existed_models):\n",
    "    repos = {}\n",
    "\n",
    "    for model in existed_models:\n",
    "        try:\n",
    "            repo_id = model['repo_id']\n",
    "\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(f\"\"\"\n",
    "                    SELECT UNIX_TIMESTAMP(created_at) FROM {MODEL_GITHUB_REPOS_TABLE_NAME}\n",
    "                    WHERE model_repo_id = '{repo_id}'\n",
    "                    ORDER BY created_at DESC\n",
    "                    LIMIT 1\n",
    "                \"\"\")\n",
    "\n",
    "                last_created_at = cursor.fetchone()\n",
    "                if (last_created_at):\n",
    "                    last_created_at = datetime.fromtimestamp(float(last_created_at[0]))\n",
    "                    last_created_at = last_created_at.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "            keyword = model['name'] if re.search(r'\\d', model['name']) else repo_id\n",
    "            found_repos = github_search_repos(keyword, last_created_at)\n",
    "\n",
    "            if not len(found_repos):\n",
    "                continue\n",
    "\n",
    "            repos[repo_id] = found_repos\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return repos\n",
    "\n",
    "\n",
    "def github_insert_models_repos(repos):\n",
    "    with connection.cursor() as cursor:\n",
    "        for model_repo_id, repos in repos.items():\n",
    "            if not len(repos):\n",
    "                continue\n",
    "\n",
    "            values = []\n",
    "\n",
    "            for repo in repos:\n",
    "                value = {\n",
    "                    'model_repo_id': model_repo_id,\n",
    "                    'repo_id': repo['repo_id'],\n",
    "                    'name': repo['name'],\n",
    "                    'description': repo['description'],\n",
    "                    'clean_text': clean_string(repo['readme']),\n",
    "                    'link': repo['link'],\n",
    "                    'created_at': repo['created_at'],\n",
    "                }\n",
    "\n",
    "                to_embedding = {\n",
    "                    'model_repo_id': model_repo_id,\n",
    "                    'name': value['name'],\n",
    "                    'description': value['description'],\n",
    "                    'clean_text': value['clean_text']\n",
    "                }\n",
    "\n",
    "                if count_tokens(value['clean_text']) <= TOKENS_TRASHHOLD_LIMIT:\n",
    "                    embedding = str(create_embedding(json.dumps(to_embedding)))\n",
    "                    values.append({**value, 'embedding': embedding})\n",
    "                else:\n",
    "                    for chunk in string_into_chunks(value['clean_text']):\n",
    "                        embedding = str(create_embedding(json.dumps({\n",
    "                            **to_embedding,\n",
    "                            'clean_text': chunk\n",
    "                        })))\n",
    "                        values.append({**value, 'clean_text': chunk, 'embedding': embedding})\n",
    "\n",
    "            cursor.executemany(f'''\n",
    "                INSERT INTO {MODEL_GITHUB_REPOS_TABLE_NAME} (model_repo_id, repo_id, name, description, clean_text, link, created_at, embedding)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, FROM_UNIXTIME(%s), JSON_ARRAY_PACK(%s))\n",
    "            ''', [list(value.values()) for value in values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = tweepy.Client(TWITTER_BEARER_TOKEN)\n",
    "\n",
    "\n",
    "def twitter_search_posts(keyword, last_created_at):\n",
    "    posts = []\n",
    "\n",
    "    try:\n",
    "        tweets = twitter.search_recent_tweets(\n",
    "            query=f'{keyword} -is:retweet',\n",
    "            tweet_fields=['id', 'text', 'created_at'],\n",
    "            start_time=last_created_at,\n",
    "            max_results=10\n",
    "        )\n",
    "\n",
    "        for tweet in tweets.data:\n",
    "            posts.append({\n",
    "                'post_id': tweet.id,\n",
    "                'text': tweet.text,\n",
    "                'created_at': tweet.created_at,\n",
    "            })\n",
    "    except Exception:\n",
    "        return posts\n",
    "\n",
    "    return posts\n",
    "\n",
    "\n",
    "def twitter_get_models_posts(existed_models):\n",
    "    posts = {}\n",
    "\n",
    "    for model in existed_models:\n",
    "        try:\n",
    "            repo_id = model['repo_id']\n",
    "\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(f\"\"\"\n",
    "                    SELECT UNIX_TIMESTAMP(created_at) FROM {MODEL_TWITTER_POSTS_TABLE_NAME}\n",
    "                    WHERE model_repo_id = '{repo_id}'\n",
    "                    ORDER BY created_at DESC\n",
    "                    LIMIT 1\n",
    "                \"\"\")\n",
    "\n",
    "                last_crated_at = cursor.fetchone()\n",
    "                if (last_crated_at):\n",
    "                    last_crated_at = datetime.fromtimestamp(float(last_crated_at[0]))\n",
    "                    last_crated_at = last_crated_at.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "            keyword = model['name'] if re.search(r'\\d', model['name']) else repo_id\n",
    "            found_posts = twitter_search_posts(keyword, last_crated_at)\n",
    "\n",
    "            if not len(found_posts):\n",
    "                continue\n",
    "\n",
    "            posts[repo_id] = found_posts\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    return posts\n",
    "\n",
    "\n",
    "def twitter_insert_models_posts(posts):\n",
    "    with connection.cursor() as cursor:\n",
    "        for model_repo_id, posts in posts.items():\n",
    "            if not len(posts):\n",
    "                continue\n",
    "\n",
    "            values = []\n",
    "\n",
    "            for post in posts:\n",
    "                value = {\n",
    "                    'model_repo_id': model_repo_id,\n",
    "                    'post_id': post['post_id'],\n",
    "                    'clean_text': clean_string(post['text']),\n",
    "                    'created_at': post['created_at'],\n",
    "                }\n",
    "\n",
    "                to_embedding = {\n",
    "                    'model_repo_id': value['model_repo_id'],\n",
    "                    'clean_text': value['clean_text']\n",
    "                }\n",
    "\n",
    "                embedding = str(create_embedding(json.dumps(to_embedding)))\n",
    "                values.append({**value, 'embedding': embedding})\n",
    "\n",
    "            cursor.executemany(f'''\n",
    "                INSERT INTO {MODEL_TWITTER_POSTS_TABLE_NAME} (model_repo_id, post_id, clean_text, created_at, embedding)\n",
    "                VALUES (%s, %s, %s, FROM_UNIXTIME(%s), JSON_ARRAY_PACK(%s))\n",
    "            ''', [list(value.values()) for value in values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.reddit.com/prefs/apps\n",
    "reddit = praw.Reddit(\n",
    "    username=REDDIT_USERNAME,\n",
    "    password=REDDIT_PASSWORD,\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT\n",
    ")\n",
    "\n",
    "\n",
    "def reddit_search_posts(keyword: str, latest_post_timestamp):\n",
    "    posts = []\n",
    "\n",
    "    # https://www.reddit.com/dev/api/#GET_search\n",
    "    # https://praw.readthedocs.io/en/stable/code_overview/models/subreddit.html#praw.models.Subreddit.search\n",
    "    for post in reddit.subreddit('all').search(\n",
    "            f'\"{keyword}\"', sort='relevance', time_filter='year', limit=100\n",
    "    ):\n",
    "        contains_keyword = keyword in post.title or keyword in post.selftext\n",
    "\n",
    "        if contains_keyword and not post.over_18:\n",
    "            if not latest_post_timestamp or (post.created_utc > latest_post_timestamp):\n",
    "                posts.append({\n",
    "                    'post_id': post.id,\n",
    "                    'title': post.title,\n",
    "                    'text': post.selftext,\n",
    "                    'link': f'https://www.reddit.com{post.permalink}',\n",
    "                    'created_at': post.created_utc,\n",
    "                })\n",
    "\n",
    "    return posts\n",
    "\n",
    "\n",
    "def reddit_get_models_posts(existed_models):\n",
    "    posts = {}\n",
    "\n",
    "    for model in existed_models:\n",
    "        repo_id = model['repo_id']\n",
    "\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT UNIX_TIMESTAMP(created_at) FROM {MODEL_REDDIT_POSTS_TABLE_NAME}\n",
    "                WHERE model_repo_id = '{repo_id}'\n",
    "                ORDER BY created_at DESC\n",
    "                LIMIT 1\n",
    "            \"\"\")\n",
    "\n",
    "            latest_post_timestamp = cursor.fetchone()\n",
    "            latest_post_timestamp = float(latest_post_timestamp[0]) if latest_post_timestamp != None else None\n",
    "\n",
    "        keyword = model['name'] if re.search(r'\\d', model['name']) else repo_id\n",
    "        found_posts = reddit_search_posts(keyword, latest_post_timestamp)\n",
    "\n",
    "        if not len(found_posts):\n",
    "            continue\n",
    "\n",
    "        posts[repo_id] = found_posts\n",
    "\n",
    "    return posts\n",
    "\n",
    "\n",
    "def reddit_insert_models_posts(posts):\n",
    "    if not len(posts):\n",
    "        return\n",
    "\n",
    "    with connection.cursor() as cursor:\n",
    "        for model_repo_id, posts in posts.items():\n",
    "            if not len(posts):\n",
    "                continue\n",
    "\n",
    "            values = []\n",
    "\n",
    "            for post in posts:\n",
    "                value = {\n",
    "                    'model_repo_id': model_repo_id,\n",
    "                    'post_id': post['post_id'],\n",
    "                    'title': post['title'],\n",
    "                    'clean_text': clean_string(post['text']),\n",
    "                    'link': post['link'],\n",
    "                    'created_at': post['created_at'],\n",
    "                }\n",
    "\n",
    "                to_embedding = {\n",
    "                    'model_repo_id': model_repo_id,\n",
    "                    'title': value['title'],\n",
    "                    'clean_text': value['clean_text']\n",
    "                }\n",
    "\n",
    "                if count_tokens(value['clean_text']) <= TOKENS_TRASHHOLD_LIMIT:\n",
    "                    embedding = str(create_embedding(json.dumps(to_embedding)))\n",
    "                    values.append({**value, 'embedding': embedding})\n",
    "                else:\n",
    "                    for chunk in string_into_chunks(value['clean_text']):\n",
    "                        embedding = str(create_embedding(json.dumps({\n",
    "                            **to_embedding,\n",
    "                            'clean_text': chunk\n",
    "                        })))\n",
    "                        values.append({**value, 'clean_text': chunk, 'embedding': embedding})\n",
    "\n",
    "            cursor.executemany(f'''\n",
    "                INSERT INTO {MODEL_REDDIT_POSTS_TABLE_NAME} (model_repo_id, post_id, title, clean_text, link, created_at, embedding)\n",
    "                VALUES (%s, %s, %s, %s, %s, FROM_UNIXTIME(%s), JSON_ARRAY_PACK(%s))\n",
    "            ''', [list(value.values()) for value in values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute scheduled logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tables()\n",
    "\n",
    "leaderboard_models = leaderboard_get_models()\n",
    "leaderboard_insert_models(leaderboard_models)\n",
    "\n",
    "existed_models = get_models('repo_id, name', 'ORDER BY score DESC')\n",
    "\n",
    "models_github_repos = github_get_models_repos(existed_models)\n",
    "github_insert_models_repos(models_github_repos)\n",
    "\n",
    "models_witter_posts = twitter_get_models_posts(existed_models)\n",
    "twitter_insert_models_posts(models_witter_posts)\n",
    "\n",
    "models_reddit_posts = reddit_get_models_posts(existed_models)\n",
    "reddit_insert_models_posts(models_reddit_posts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
